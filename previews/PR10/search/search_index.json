{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Boltz.jl","text":""},{"location":"#boltz","title":"Boltz \u26a1","text":"<p>Accelerate \u26a1 your ML research using pre-built Deep Learning Models with Lux</p> <p></p> <p></p>"},{"location":"#installation","title":"Installation","text":"<pre><code>using Pkg\nPkg.add(\"Boltz\")\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<pre><code>using Boltz, Lux\n\nmodel, ps, st = resnet(:resnet18; pretrained=true)\n</code></pre>"},{"location":"#changelog","title":"Changelog","text":""},{"location":"#updating-from-v01-to-v02","title":"Updating from v0.1 to v0.2","text":"<p>We have moved some dependencies into weak dependencies. This means that you will have to manually load them for certain features to be available.</p> <ul> <li>To load Flux &amp; Metalhead models, do <code>using Flux, Metalhead</code>.</li> </ul>"},{"location":"api/vision/","title":"Computer Vision","text":""},{"location":"api/vision/#computer-vision-models","title":"Computer Vision Models","text":"<ul> <li><code>Boltz.ClassTokens</code></li> <li><code>Boltz.MultiHeadAttention</code></li> <li><code>Boltz.ViPosEmbedding</code></li> <li><code>Boltz._fast_chunk</code></li> <li><code>Boltz._flatten_spatial</code></li> <li><code>Boltz._seconddimmean</code></li> <li><code>Boltz._vgg_block</code></li> <li><code>Boltz._vgg_classifier_layers</code></li> <li><code>Boltz._vgg_convolutional_layers</code></li> <li><code>Boltz.transformer_encoder</code></li> <li><code>Boltz.vgg</code></li> </ul>"},{"location":"api/vision/#classification-models-native-lux-models","title":"Classification Models: Native Lux Models","text":"MODEL NAME FUNCTION NAME PRETRAINED TOP 1 ACCURACY (%) TOP 5 ACCURACY (%) VGG <code>vgg</code> <code>:vgg11</code> \u2705 67.35 87.91 VGG <code>vgg</code> <code>:vgg13</code> \u2705 68.40 88.48 VGG <code>vgg</code> <code>:vgg16</code> \u2705 70.24 89.80 VGG <code>vgg</code> <code>:vgg19</code> \u2705 71.09 90.27 VGG <code>vgg</code> <code>:vgg11_bn</code> \u2705 69.09 88.94 VGG <code>vgg</code> <code>:vgg13_bn</code> \u2705 69.66 89.49 VGG <code>vgg</code> <code>:vgg16_bn</code> \u2705 72.11 91.02 VGG <code>vgg</code> <code>:vgg19_bn</code> \u2705 72.95 91.32 Vision Transformer <code>vision_transformer</code> <code>:tiny</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:small</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:base</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:large</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:huge</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:giant</code> \ud83d\udeab Vision Transformer <code>vision_transformer</code> <code>:gigantic</code> \ud83d\udeab"},{"location":"api/vision/#building-blocks","title":"Building Blocks","text":"<p># <code>Boltz.ClassTokens</code> \u2014 Type.</p> <pre><code>ClassTokens(dim; init=Lux.zeros32)\n</code></pre> <p>Appends class tokens to an input with embedding dimension <code>dim</code> for use in many vision transformer namels.</p> <p>source</p> <p># <code>Boltz.MultiHeadAttention</code> \u2014 Type.</p> <pre><code>MultiHeadAttention(in_planes::Int, number_heads::Int; qkv_bias::Bool=false,\nattention_dropout_rate::T=0.0f0,\nprojection_dropout_rate::T=0.0f0) where {T}\n</code></pre> <p>Multi-head self-attention layer</p> <p>source</p> <p># <code>Boltz.ViPosEmbedding</code> \u2014 Type.</p> <pre><code>ViPosEmbedding(embedsize, npatches;\ninit = (rng, dims...) -&gt; randn(rng, Float32, dims...))\n</code></pre> <p>Positional embedding layer used by many vision transformer-like namels.</p> <p>source</p> <p># <code>Boltz.transformer_encoder</code> \u2014 Function.</p> <pre><code>transformer_encoder(in_planes, depth, number_heads; mlp_ratio = 4.0f0, dropout = 0.0f0)\n</code></pre> <p>Transformer as used in the base ViT architecture. (reference).</p> <p>Arguments</p> <ul> <li><code>in_planes</code>: number of input channels</li> <li><code>depth</code>: number of attention blocks</li> <li><code>number_heads</code>: number of attention heads</li> <li><code>mlp_ratio</code>: ratio of MLP layers to the number of input channels</li> <li><code>dropout_rate</code>: dropout rate</li> </ul> <p>source</p> <p># <code>Boltz.vgg</code> \u2014 Function.</p> <pre><code>vgg(imsize; config, inchannels, batchnorm = false, nclasses, fcsize, dropout)\n</code></pre> <p>Create a VGG model (reference).</p> <p>Arguments</p> <ul> <li><code>imsize</code>: input image width and height as a tuple</li> <li><code>config</code>: the configuration for the convolution layers</li> <li><code>inchannels</code>: number of input channels</li> <li><code>batchnorm</code>: set to <code>true</code> to use batch normalization after each convolution</li> <li><code>nclasses</code>: number of output classes</li> <li><code>fcsize</code>: intermediate fully connected layer size (see <code>Metalhead._vgg_classifier_layers</code>)</li> <li><code>dropout</code>: dropout level between fully connected layers</li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/vision/#non-public-api","title":"Non-Public API","text":"<p># <code>Boltz._seconddimmean</code> \u2014 Function.</p> <pre><code>_seconddimmean(x)\n</code></pre> <p>Computes the mean of <code>x</code> along dimension <code>2</code></p> <p>source</p> <p># <code>Boltz._fast_chunk</code> \u2014 Function.</p> <pre><code>_fast_chunk(x::AbstractArray, ::Val{n}, ::Val{dim})\n</code></pre> <p>Type-stable and faster version of <code>MLUtils.chunk</code></p> <p>source</p> <p># <code>Boltz._flatten_spatial</code> \u2014 Function.</p> <pre><code>_flatten_spatial(x::AbstractArray{T, 4})\n</code></pre> <p>Flattens the first 2 dimensions of <code>x</code>, and permutes the remaining dimensions to (2, 1, 3)</p> <p>source</p> <p># <code>Boltz._vgg_block</code> \u2014 Function.</p> <pre><code>_vgg_block(input_filters, output_filters, depth, batchnorm)\n</code></pre> <p>A VGG block of convolution layers (reference).</p> <p>Arguments</p> <ul> <li><code>input_filters</code>: number of input feature maps</li> <li><code>output_filters</code>: number of output feature maps</li> <li><code>depth</code>: number of convolution/convolution + batch norm layers</li> <li><code>batchnorm</code>: set to <code>true</code> to include batch normalization after each convolution</li> </ul> <p>source</p> <p># <code>Boltz._vgg_classifier_layers</code> \u2014 Function.</p> <pre><code>_vgg_classifier_layers(imsize, nclasses, fcsize, dropout)\n</code></pre> <p>Create VGG classifier (fully connected) layers (reference).</p> <p>Arguments</p> <ul> <li><code>imsize</code>: tuple <code>(width, height, channels)</code> indicating the size after the convolution layers (see <code>Metalhead._vgg_convolutional_layers</code>)</li> <li><code>nclasses</code>: number of output classes</li> <li><code>fcsize</code>: input and output size of the intermediate fully connected layer</li> <li><code>dropout</code>: the dropout level between each fully connected layer</li> </ul> <p>source</p> <p># <code>Boltz._vgg_convolutional_layers</code> \u2014 Function.</p> <pre><code>_vgg_convolutional_layers(config, batchnorm, inchannels)\n</code></pre> <p>Create VGG convolution layers (reference).</p> <p>Arguments</p> <ul> <li><code>config</code>: vector of tuples <code>(output_channels, num_convolutions)</code> for each block (see <code>Metalhead._vgg_block</code>)</li> <li><code>batchnorm</code>: set to <code>true</code> to include batch normalization after each convolution</li> <li><code>inchannels</code>: number of input channels</li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/vision/#classification-models-imported-from-metalheadjl","title":"Classification Models: Imported from Metalhead.jl","text":"<p>Tip</p> <p>You need to load <code>Flux</code> and <code>Metalhead</code> before using these models.</p> MODEL NAME FUNCTION NAME PRETRAINED TOP 1 ACCURACY (%) TOP 5 ACCURACY (%) AlexNet <code>alexnet</code> <code>:alexnet</code> \u2705 54.48 77.72 ResNet <code>resnet</code> <code>:resnet18</code> \u2705 68.08 88.44 ResNet <code>resnet</code> <code>:resnet34</code> \u2705 72.13 90.91 ResNet <code>resnet</code> <code>:resnet50</code> \u2705 74.55 92.36 ResNet <code>resnet</code> <code>:resnet101</code> \u2705 74.81 92.36 ResNet <code>resnet</code> <code>:resnet152</code> \u2705 77.63 93.84 ConvMixer <code>convmixer</code> <code>:small</code> \ud83d\udeab ConvMixer <code>convmixer</code> <code>:base</code> \ud83d\udeab ConvMixer <code>convmixer</code> <code>:large</code> \ud83d\udeab DenseNet <code>densenet</code> <code>:densenet121</code> \ud83d\udeab DenseNet <code>densenet</code> <code>:densenet161</code> \ud83d\udeab DenseNet <code>densenet</code> <code>:densenet169</code> \ud83d\udeab DenseNet <code>densenet</code> <code>:densenet201</code> \ud83d\udeab GoogleNet <code>googlenet</code> <code>:googlenet</code> \ud83d\udeab MobileNet <code>mobilenet</code> <code>:mobilenet_v1</code> \ud83d\udeab MobileNet <code>mobilenet</code> <code>:mobilenet_v2</code> \ud83d\udeab MobileNet <code>mobilenet</code> <code>:mobilenet_v3_small</code> \ud83d\udeab MobileNet <code>mobilenet</code> <code>:mobilenet_v3_large</code> \ud83d\udeab ResNeXT <code>resnext</code> <code>:resnext50</code> \ud83d\udeab ResNeXT <code>resnext</code> <code>:resnext101</code> \ud83d\udeab ResNeXT <code>resnext</code> <code>:resnext152</code> \ud83d\udeab <p>These models can be created using <code>&lt;FUNCTION&gt;(&lt;NAME&gt;; pretrained = &lt;PRETRAINED&gt;)</code></p> <p></p> <p></p>"},{"location":"api/vision/#preprocessing","title":"Preprocessing","text":"<p>All the pretrained models require that the images be normalized with the parameters <code>mean = [0.485f0, 0.456f0, 0.406f0]</code> and <code>std = [0.229f0, 0.224f0, 0.225f0]</code>.</p>"}]}